# REALMs for Research Project

`REALMs - REtrieval Augmented Language Models` 

This repository houses a comprehensive project focused on leveraging Large Language Models (LLMs) for various tasks such as knowledge graph construction, data embedding, and visualization. The project integrates data scraping, processing, and model training, culminating in a web interface for interaction with the trained models.

## Project Structure

- **LLM/**: Main scripts and utilities for Large Language Model operations.
- **data/**: Contains raw and processed data used and generated by the project.
- **datasets/**: Storage for datasets used in model training and evaluation.
- **embed_models/**: Embedding models used for representing text data.
- **fetched_data/**: Data fetched from various sources.
- **gen_KG/**: Scripts and notebooks for generating knowledge graphs.
- **kg-nlp/**: NLP utilities specific to knowledge graph construction.
- **model/**: The main LLM models and their configurations.
- **output/eval/**: Evaluation output data for model performance analysis.
- **visualisation/**: Visualization tools and scripts for data and model outputs.
- **website/**: Web interface for interacting with the LLM models.

## Key Files

- `.gitignore`: Specifies files to be ignored in git version control.
- `README.md`: This file, describing the project and its structure.
- `clean_data.py`: Script for cleaning and preprocessing data.
- `create_eval_dataset.py`: Utility for creating datasets for model evaluation.
- `file_data.py`: Script to handle file-based data operations.
- `gen_embed_model.py`: Generates embedding models for text representation.
- `gen_pmpt.py`: Utility for generating prompts for LLMs.
- `generate_embed.py`: Script for generating data embeddings.
- `generate_kg.py`: Script for generating knowledge graphs.
- `kg-construction.ipynb`: Jupyter notebook for knowledge graph construction process.
- `scrape_article.py`: Utility for scraping articles from the web.
- `scrape_web.py`: Script for general web scraping.

## Getting Started

To get started with this project, clone the repository and install the required dependencies. Ensure you have Python and the necessary libraries installed. Follow the specific instructions in each subdirectory for running individual scripts and modules.

```bash
git clone https://github.com/your-repo/LLM.git
cd LLM
```
## Usage

Each script and directory within this project is designed for specific purposes. To understand how to use each component, please refer to the individual README files located in their respective directories. These guides will provide detailed instructions and examples to help you effectively utilize each part of the project.

## Contributing

We welcome contributions to this project! If you're interested in helping out, please follow these steps:

1. **Fork the Repository**: Create your own fork of the repo.
2. **Clone**: Clone the repository to your local machine.
3. **Create a New Branch**: Make your changes in a new git branch.
4. **Make Your Changes**: Add your changes or improvements.
5. **Commit Your Changes**: Commit your changes with a clear and descriptive commit message.
6. **Push to the Branch**: Push your changes to your fork.
7. **Submit a Pull Request**: Open a pull request from your fork to the main repo.

For major changes or new features, please open an issue first to discuss what you would like to change. This helps us to keep track of suggestions and manage the development process.

## License

This project is licensed under the MIT License. For more information, please refer to the [LICENSE.md](LICENSE.md) file included in this repository.

## Acknowledgments

A big thank you to all the contributors and maintainers of this project. Your hard work and dedication are greatly appreciated.

This project was inspired by the ongoing advancements in the field of Large Language Models and their diverse applications. We are excited to be a part of this evolving landscape and look forward to seeing how these technologies continue to develop.



